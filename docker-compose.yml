version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  init-kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    # Questo comando aspetta che Kafka sia pronto e poi crea i topic
    command: |
      "
      echo -e 'Waiting for Kafka to be ready...'
      cub kafka-ready -b kafka:9092 1 20

      echo -e 'Creating kafka topics...'
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic raw-articles --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic classified-articles --replication-factor 1 --partitions 1

      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka:9092 --list
      "
  # -----------------------------------------

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.2
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.2
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200

  logstash:
    image: docker.elastic.co/logstash/logstash:8.10.2
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    depends_on:
      - kafka
      - elasticsearch
      - init-kafka # Aspetta che i topic siano creati

  bot:
    build: ./bot
    depends_on:
      - kafka
      - elasticsearch
      - init-kafka # Aspetta che i topic siano creati
    env_file: .env
    restart: always

  spark-worker:
    build: ./spark
    depends_on:
      - kafka
      - init-kafka # FONDAMENTALE: Aspetta che i topic esistano prima di partire
    env_file: .env
    environment:
      - SPARK_SUBMIT_ARGS=--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 pyspark-shell
    deploy:
      resources:
        limits:
          memory: 6G

volumes:
  es_data: